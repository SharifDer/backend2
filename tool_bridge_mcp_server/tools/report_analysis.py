# --- START OF FILE report_analysis.py ---

import logging
import os
import sys
import json
from typing import Optional

# Add the grandparent directory to sys.path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from mcp.server.fastmcp import FastMCP
from pydantic import Field
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")))
from tool_bridge_mcp_server.context import get_app_context


logger = logging.getLogger(__name__)

def register_report_analysis_tools(mcp: FastMCP):
    """Register report analysis tool."""

    @mcp.tool(
        name="report_analysis",
        description="""Analyze saved reports and answer questions using LLM.
        
        üîç Analysis Capabilities:
        - Read and understand markdown report content
        - Answer specific questions about the report
        - Provide insights and explanations
        - Extract key findings and metrics
        
        üìÑ Supported Report Types:
        - Territory optimization reports
        - Hub expansion analysis reports
        - Any markdown report generated by the system
        
        ‚ö° Usage:
        - Takes a report file handle/path and user question
        - Returns LLM-generated analysis and answers
        """,
    )
    async def report_analysis(
        report_file: str = Field(description="Path to the saved report file (markdown)"),
        user_query: str = Field(description="Question or analysis request about the report"),
        model: str = Field(default="gpt-4.1", description="LLM model to use for analysis"),
        temperature: float = Field(default=0.0, description="Temperature for LLM responses (0.0-1.0)")
    ) -> str:
        """Analyze saved reports and answer questions using LLM."""
        
        print("üîç [REPORT_ANALYSIS] Starting report analysis...")
        print(f"üîç [REPORT_ANALYSIS] Report file: {report_file}")
        print(f"üîç [REPORT_ANALYSIS] User query: {user_query}")
        print(f"üîç [REPORT_ANALYSIS] Model: {model}, Temperature: {temperature}")
        
        try:
            print("üîç [REPORT_ANALYSIS] Getting app context...")
            app_ctx = get_app_context(mcp)
            session_manager = app_ctx.session_manager
            print("‚úÖ [REPORT_ANALYSIS] App context retrieved successfully")

            # Authentication check
            print("üîç [REPORT_ANALYSIS] Checking authentication...")
            user_id, id_token = await session_manager.get_valid_id_token()
            if not id_token or not user_id:
                print("‚ùå [REPORT_ANALYSIS] Authentication failed - user not logged in")
                return "‚ùå Error: You are not logged in. Please use the `user_login` tool first."
            print(f"‚úÖ [REPORT_ANALYSIS] Authentication successful for user: {user_id}")

            # Read the report file
            print(f"üîç [REPORT_ANALYSIS] Reading report file: {report_file}")
            report_content = read_report_file(report_file)
            if not report_content:
                print(f"‚ùå [REPORT_ANALYSIS] Failed to read report file: {report_file}")
                return f"‚ùå Error: Could not read report file: {report_file}"
            
            report_length = len(report_content)
            print(f"‚úÖ [REPORT_ANALYSIS] Report file read successfully ({report_length} characters)")

            logger.info(f"Analyzing report for user {user_id}: {report_file}")
            logger.info(f"User query: {user_query[:100]}...")

            # Initialize LLM (uses global GEMINI_API_KEY environment variable)
            print(f"üîç [REPORT_ANALYSIS] Initializing LLM with model: {model}")
            llm = ChatOpenAI(model=model, temperature=temperature)
            print("‚úÖ [REPORT_ANALYSIS] LLM initialized successfully")
            
            # Create system prompt for report analysis
            print("üîç [REPORT_ANALYSIS] Creating system prompt and messages...")
            system_prompt = f"""You are an expert analyst reviewing a comprehensive report. 
Your task is to analyze the provided report content and answer the user's question with accuracy and insight.

Report Analysis Guidelines:
- Focus on the specific information requested in the user's query
- Provide detailed, data-driven answers when possible
- Reference specific sections, metrics, or findings from the report
- If the report doesn't contain information needed to answer the query, clearly state this
- Maintain a professional, analytical tone

Report Content:
{report_content}
"""

            # Create messages
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_query)
            ]
            print(f"‚úÖ [REPORT_ANALYSIS] Messages created (system prompt: {len(system_prompt)} chars)")

            # Get LLM response
            print("üîç [REPORT_ANALYSIS] Calling LLM for analysis...")
            response = llm.invoke(messages)
            print("‚úÖ [REPORT_ANALYSIS] LLM response received")
            print(f"üîç [REPORT_ANALYSIS] Response length: {len(response.content)} characters")
            
            logger.info(f"Successfully generated analysis for report: {report_file}")
            print("‚úÖ [REPORT_ANALYSIS] Analysis completed successfully")
            return response.content

        except Exception as e:
            print(f"‚ùå [REPORT_ANALYSIS] Critical error occurred: {str(e)}")
            logger.exception("Critical error in report_analysis")
            import traceback
            traceback.print_exc()
            return f"‚ùå Error analyzing report: {str(e)}"


def read_report_file(file_path: str) -> Optional[str]:
    """
    Read report content from file path.
    
    Args:
        file_path: Path to the report file
        
    Returns:
        Content of the report file or None if error
    """
    print(f"üîç [READ_REPORT] Starting to read file: {file_path}")
    
    try:
        original_path = file_path
        
        # Handle both absolute paths and relative paths
        if not os.path.isabs(file_path):
            print(f"üîç [READ_REPORT] File path is relative, converting to absolute...")
            # If relative path, assume it's in the reports directory
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(current_dir))
            file_path = os.path.join(project_root, "reports", file_path)
            print(f"üîç [READ_REPORT] Converted path: {file_path}")
        else:
            print(f"üîç [READ_REPORT] File path is already absolute")
        
        print(f"üîç [READ_REPORT] Checking if file exists: {file_path}")
        if not os.path.exists(file_path):
            print(f"‚ùå [READ_REPORT] File not found: {file_path}")
            logger.error(f"Report file not found: {file_path}")
            return None
        print(f"‚úÖ [READ_REPORT] File exists")
            
        print(f"üîç [READ_REPORT] Checking file extension...")
        if not file_path.endswith('.md'):
            print(f"‚ùå [READ_REPORT] File is not markdown: {file_path}")
            logger.error(f"File is not a markdown file: {file_path}")
            return None
        print(f"‚úÖ [READ_REPORT] File has .md extension")
            
        print(f"üîç [READ_REPORT] Opening and reading file content...")
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        content_length = len(content)
        print(f"‚úÖ [READ_REPORT] Successfully read file ({content_length} characters)")
        logger.info(f"Successfully read report file: {file_path}")
        return content
        
    except Exception as e:
        print(f"‚ùå [READ_REPORT] Error reading file {original_path}: {str(e)}")
        logger.error(f"Error reading report file {file_path}: {str(e)}")
        import traceback
        traceback.print_exc()
        return None